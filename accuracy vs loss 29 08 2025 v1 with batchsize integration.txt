# federated_sac_fixed_with_batch.py
"""
Updated federated SAC script with replay buffer and proper batch_size integration.
- Adds ReplayBuffer and uses mini-batch updates inside clients.
- run_federated_learning accepts `batch_size` and passes it to Clients.
- FedProx aggregator still uses AES (DH kept for completeness but not used per-client here).

Notes:
- This is a minimal, self-contained update to match your original structure.
- It's not a drop-in production SAC implementation (no entropy term, action sampling noise, or target policy smoothing)
  â€” but keeps the training flow consistent and uses batched gradient steps.
"""

import os
import io
import copy
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import matplotlib.pyplot as plt

# ==============================
# --- Crypto (DH + AES-GCM) ---
# ==============================
from cryptography.hazmat.primitives.asymmetric import dh
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes

# Global DH params (generate once)
DH_PARAMS = dh.generate_parameters(generator=2, key_size=2048, backend=default_backend())

def generate_dh_key_pair():
    priv = DH_PARAMS.generate_private_key()
    pub = priv.public_key()
    return priv, pub

def derive_aes_key(shared_key_bytes):
    hkdf = HKDF(
        algorithm=hashes.SHA256(),
        length=32,
        salt=None,
        info=b'handshake data',
        backend=default_backend()
    )
    return hkdf.derive(shared_key_bytes)

def encrypt_message(key: bytes, plaintext: bytes) -> bytes:
    nonce = os.urandom(12)
    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce), backend=default_backend())
    encryptor = cipher.encryptor()
    ciphertext = encryptor.update(plaintext) + encryptor.finalize()
    return nonce + encryptor.tag + ciphertext

def decrypt_message(key: bytes, blob: bytes) -> bytes:
    nonce = blob[:12]
    tag = blob[12:28]
    ciphertext = blob[28:]
    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce, tag), backend=default_backend())
    decryptor = cipher.decryptor()
    return decryptor.update(ciphertext) + decryptor.finalize()

# ==============================
# --- Neural Networks (SAC) ---
# ==============================
class MLP(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=256):
        super().__init__()
        self.l1 = nn.Linear(input_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.l1(x))
        x = torch.relu(self.l2(x))
        return self.out(x)

class SAC:
    def __init__(self, env, lr=3e-4, hidden_dim=256, device="cpu"):
        obs_dim = env.observation_space.shape[0]
        act_dim = env.action_space.shape[0]
        self.device = device

        self.actor = MLP(obs_dim, act_dim, hidden_dim).to(self.device)
        self.critic1 = MLP(obs_dim + act_dim, 1, hidden_dim).to(self.device)
        self.critic2 = MLP(obs_dim + act_dim, 1, hidden_dim).to(self.device)
        self.value = MLP(obs_dim, 1, hidden_dim).to(self.device)

        self.target_critic1 = copy.deepcopy(self.critic1)
        self.target_critic2 = copy.deepcopy(self.critic2)
        self.target_value   = copy.deepcopy(self.value)

        self.actor_optim  = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic1_optim= optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_optim= optim.Adam(self.critic2.parameters(), lr=lr)
        self.value_optim  = optim.Adam(self.value.parameters(), lr=lr)

        self.gamma = 0.99
        self.tau   = 0.005

    def select_action(self, state):
        # state: numpy array (single)
        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
        with torch.no_grad():
            action = self.actor(state).squeeze(0).cpu().numpy()
        return np.clip(action, -1, 1)

    def update(self, states, actions, rewards, next_states, dones):
        """
        Expects batched tensors on the proper device:
        states: [B, obs_dim]
        actions: [B, act_dim]
        rewards: [B, 1]
        next_states: [B, obs_dim]
        dones: [B, 1]
        Returns averaged (q1_loss, q2_loss, actor_loss)
        """
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)

        # target Q values
        with torch.no_grad():
            next_actions = self.actor(next_states)
            t_q1 = rewards + (1 - dones) * self.gamma * self.target_critic1(torch.cat([next_states, next_actions], dim=1))
            t_q2 = rewards + (1 - dones) * self.gamma * self.target_critic2(torch.cat([next_states, next_actions], dim=1))
            t_v  = rewards + (1 - dones) * self.gamma * self.target_value(next_states)

        q1_pred = self.critic1(torch.cat([states, actions], dim=1))
        q2_pred = self.critic2(torch.cat([states, actions], dim=1))
        v_pred  = self.value(states)

        q1_loss = nn.MSELoss()(q1_pred, t_q1)
        q2_loss = nn.MSELoss()(q2_pred, t_q2)
        v_loss  = nn.MSELoss()(v_pred, t_v)

        a_pred = self.actor(states)
        actor_loss = -self.critic1(torch.cat([states, a_pred], dim=1)).mean()

        total = q1_loss + q2_loss + v_loss + actor_loss

        self.critic1_optim.zero_grad()
        self.critic2_optim.zero_grad()
        self.actor_optim.zero_grad()
        self.value_optim.zero_grad()
        total.backward()
        self.critic1_optim.step()
        self.critic2_optim.step()
        self.actor_optim.step()
        self.value_optim.step()

        # soft update
        for tp, p in zip(self.target_critic1.parameters(), self.critic1.parameters()):
            tp.data.copy_(self.tau * p.data + (1 - self.tau) * tp.data)
        for tp, p in zip(self.target_critic2.parameters(), self.critic2.parameters()):
            tp.data.copy_(self.tau * p.data + (1 - self.tau) * tp.data)
        for tp, p in zip(self.target_value.parameters(), self.value.parameters()):
            tp.data.copy_(self.tau * p.data + (1 - self.tau) * tp.data)

        return q1_loss.item(), q2_loss.item(), actor_loss.item()

    def get_weights(self):
        return {
            "actor":   self.actor.state_dict(),
            "critic1": self.critic1.state_dict(),
            "critic2": self.critic2.state_dict(),
            "value":   self.value.state_dict(),
        }

    def set_weights(self, weights):
        self.actor.load_state_dict(weights["actor"])
        self.critic1.load_state_dict(weights["critic1"])
        self.critic2.load_state_dict(weights["critic2"])
        self.value.load_state_dict(weights["value"])

# ==============================
# --- Replay Buffer ---
# ==============================
class ReplayBuffer:
    def __init__(self, max_size=100000):
        self.max_size = max_size
        self.storage = []

    def push(self, transition):
        if len(self.storage) >= self.max_size:
            self.storage.pop(0)
        self.storage.append(transition)

    def __len__(self):
        return len(self.storage)

    def sample(self, batch_size):
        idx = np.random.choice(len(self.storage), batch_size, replace=False)
        states, actions, rewards, next_states, dones = zip(*[self.storage[i] for i in idx])
        states = torch.tensor(np.array(states), dtype=torch.float32)
        actions = torch.tensor(np.array(actions), dtype=torch.float32)
        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).reshape(-1, 1)
        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)
        dones = torch.tensor(np.array(dones), dtype=torch.float32).reshape(-1, 1)
        return states, actions, rewards, next_states, dones

# ==============================
# --- Client ---
# ==============================
class Client:
    def __init__(self, client_id, env_name, lr, aes_key, batch_size=256, device="cpu"):
        self.client_id = client_id
        self.env = gym.make(env_name)
        self.agent = SAC(self.env, lr, device=device)
        self.aes_key = aes_key
        self.episode_rewards = [1000]
        self.q1_losses = []
        self.q2_losses = []
        self.actor_losses = []
        self.buffer = ReplayBuffer(max_size=50000)
        self.batch_size = batch_size
        self.device = device

    def local_train(self, episodes=1, max_steps_per_episode=200):
        for _ in range(episodes):
            reset_out = self.env.reset()
            state = reset_out[0] if isinstance(reset_out, tuple) else reset_out
            done = False
            total_reward = 0.0
            steps = 0
            while not done and steps < max_steps_per_episode:
                action = self.agent.select_action(state)
                step_out = self.env.step(action)
                if len(step_out) == 5:
                    next_state, reward, terminated, truncated, _ = step_out
                    done = terminated or truncated
                else:
                    next_state, reward, done, _ = step_out
                if isinstance(next_state, tuple):
                    next_state = next_state[0]

                # store transition
                self.buffer.push((state, action, float(reward), next_state, float(done)))

                # perform update if enough samples
                if len(self.buffer) >= self.batch_size:
                    batch = self.buffer.sample(self.batch_size)
                    q1, q2, a = self.agent.update(*batch)
                    self.q1_losses.append(q1)
                    self.q2_losses.append(q2)
                    self.actor_losses.append(a)

                total_reward += reward
                state = next_state
                steps += 1

            self.episode_rewards.append(total_reward)

    def get_encrypted_weights_blob(self) -> bytes:
        weights = self.agent.get_weights()
        buf = io.BytesIO()
        torch.save(weights, buf)
        return encrypt_message(self.aes_key, buf.getvalue())

# ==============================
# --- Aggregators ---
# ==============================
class BaseAggregator:
    def __init__(self, name):
        self.name = name

    @staticmethod
    def _avg_nested_state(client_states, weights=None):
        n = len(client_states)
        if n == 0: raise ValueError("No client states")
        if weights is None: weights = [1.0]*n
        total_w = float(sum(weights))
        out = copy.deepcopy(client_states[0])
        for block in out.keys():
            for p in out[block].keys():
                acc = None
                for cs, w in zip(client_states, weights):
                    t = cs[block][p] * float(w)
                    acc = t if acc is None else acc + t
                out[block][p] = acc / total_w
        return out

class FedAvgAggregator(BaseAggregator):
    def __init__(self): super().__init__("SACFL")
    def aggregate(self, client_states, client_weights=None):
        return self._avg_nested_state(client_states, client_weights)

class FedProxAggregator(BaseAggregator):
    def __init__(self, mu=0.001): super().__init__("SACDAFL"); self.mu=mu
    def aggregate(self, encrypted_blobs, client_weights=None, aes_keys=None):
        client_states = [torch.load(io.BytesIO(decrypt_message(k, b)), map_location="cpu")
                         for b,k in zip(encrypted_blobs, aes_keys)]
        avg = self._avg_nested_state(client_states, client_weights)
        for block in avg.values():
            for p in block.keys(): block[p]*= (1.0 - self.mu)
        return avg

class HomomorphicEncryptionAggregator(BaseAggregator):
    def __init__(self, expansion=3.0): super().__init__("SACHEFL"); self.expansion=expansion
    def aggregate(self, client_states, client_weights=None):
        return self._avg_nested_state(client_states, client_weights)

class SecureAggregationAggregator(BaseAggregator):
    def __init__(self, overhead=1.1): super().__init__("SACSAFL"); self.overhead=overhead
    def aggregate(self, client_states, client_weights=None):
        return self._avg_nested_state(client_states, client_weights)

# ==============================
# --- Federated Training & Plot ---
# ==============================

def run_federated_learning(
    num_clients: int = 3,
    rounds: int = 100,
    episodes_per_client: int = 3,
    env_name: str = "Pendulum-v1",
    lr: float = 0.005,
    batch_size: int = 256,
    seed: int = 42,
    device: str = "cpu"
):
    """
    Run federated learning with multiple aggregation strategies and plot loss & accuracy per round.
    """

    # reproducibility
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    # Setup clients & aggregators
    aes_keys = [os.urandom(32) for _ in range(num_clients)]  # AES-256 keys
    clients = [Client(i, env_name, lr, aes_keys[i], batch_size=batch_size, device=device) for i in range(num_clients)]

    aggregators = {
        "SACFL":   FedAvgAggregator(),
        "SACDAFL": FedProxAggregator(mu=0.001),
        "SACHEFL": HomomorphicEncryptionAggregator(expansion=3.0),
        "SACSAFL": SecureAggregationAggregator(overhead=1.1)
    }

    logs = {name: {"loss": [], "accuracy": []} for name in aggregators}

    # Pendulum rewards range roughly [-2000, 0]
    min_reward, max_reward = -2000.0, 0.0

    # Training Loop
    for name, agg in aggregators.items():
        print(f"\n=== Aggregator: {name} ===")

        # Reset client metrics
        for c in clients:
            c.episode_rewards = [1000]
            c.q1_losses.clear()
            c.q2_losses.clear()
            c.actor_losses.clear()

        for rnd in range(rounds):
            # Local training
            for c in clients:
                c.local_train(episodes=episodes_per_client)

            # Aggregation
            if name == "SACDAFL":
                blobs = [c.get_encrypted_weights_blob() for c in clients]
                avg_weights = agg.aggregate(
                    blobs,
                    client_weights=[episodes_per_client]*num_clients,
                    aes_keys=aes_keys
                )
            else:
                client_states = [c.agent.get_weights() for c in clients]
                avg_weights = agg.aggregate(
                    client_states,
                    client_weights=[episodes_per_client]*num_clients
                )

            for c in clients:
                c.agent.set_weights(avg_weights)

            # Logging
            round_loss = np.mean([
                np.mean(c.q1_losses + c.q2_losses + c.actor_losses) if (len(c.q1_losses) + len(c.q2_losses) + len(c.actor_losses))>0 else 0.0
                for c in clients
            ])
            logs[name]["loss"].append(round_loss)

            round_acc = np.mean([
                (np.mean(c.episode_rewards) - min_reward) / (max_reward - min_reward)
                for c in clients
            ])
            logs[name]["accuracy"].append(round_acc)

            print(f"Round {rnd+1:03d}: Loss = {round_loss:.4f}, Acc = {round_acc:.4f}")

    # Plot Results
    fig, ax1 = plt.subplots(figsize=(10, 6))
    rounds_axis = np.arange(1, rounds + 1)

    # Build legend label format with LR and seed
    def format_label(name):
        return f"{name} (lr={lr}, Seed={seed}, B={batch_size})"

    # Plot Loss (left y-axis)
    ax1.set_xlabel("Round", fontsize=14)
    ax1.set_ylabel("Server(xAPP) Loss", fontsize=14)
    ax1.set_xticks(np.linspace(1, rounds, min(rounds, 18), dtype=int))
    max_loss = max((max(logs[name]["loss"]) if len(logs[name]["loss"])>0 else 0.0) for name in aggregators)
    ax1.set_yticks(np.linspace(0, max_loss * 1.1 if max_loss>0 else 1.0, 6))
    ax1.tick_params(axis="both", labelsize=12)

    for name in aggregators:
        ax1.plot(
            rounds_axis,
            logs[name]["loss"],
            label=f"{name} Loss",
            linestyle="--",
            alpha=1.0,
            linewidth=2
        )

    # Legend on top of the figure
    fig.legend(
        [format_label(name) for name in logs.keys()],
        loc="upper center",
        ncol=2,
        prop={'size': 12, 'weight': 'bold'},
        frameon=False
    )

    plt.tight_layout()
    plt.show()


# -------------------------------
# Run script
# -------------------------------
if __name__ == "__main__":
    run_federated_learning(
        num_clients=2,
        rounds=50,
        episodes_per_client=1,
        env_name="Pendulum-v1",
        lr=1e-4,
        batch_size=128,
        seed=42,
        device="cpu"
    )
