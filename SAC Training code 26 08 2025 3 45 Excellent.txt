# federated_sac.py
import os
import io
import copy
import pickle
import random
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import matplotlib.pyplot as plt

# ==============================
# --- Crypto (DH + AES-GCM) ---
# ==============================
from cryptography.hazmat.primitives.asymmetric import dh
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes

# Global DH params (generate once)
DH_PARAMS = dh.generate_parameters(generator=2, key_size=2048, backend=default_backend())

def generate_dh_key_pair():
    priv = DH_PARAMS.generate_private_key()
    pub = priv.public_key()
    return priv, pub

def derive_aes_key(shared_key_bytes):
    hkdf = HKDF(
        algorithm=hashes.SHA256(),
        length=32,
        salt=None,
        info=b'handshake data',
        backend=default_backend()
    )
    return hkdf.derive(shared_key_bytes)

def encrypt_message(key: bytes, plaintext: bytes) -> bytes:
    nonce = os.urandom(12)
    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce), backend=default_backend())
    encryptor = cipher.encryptor()
    ciphertext = encryptor.update(plaintext) + encryptor.finalize()
    # Concatenate: [nonce | tag | ciphertext]
    return nonce + encryptor.tag + ciphertext

def decrypt_message(key: bytes, blob: bytes) -> bytes:
    nonce = blob[:12]
    tag = blob[12:28]
    ciphertext = blob[28:]
    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce, tag), backend=default_backend())
    decryptor = cipher.decryptor()
    return decryptor.update(ciphertext) + decryptor.finalize()

# ==============================
# --- Neural Networks (SAC) ---
# ==============================
class MLP(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=256):
        super().__init__()
        self.l1 = nn.Linear(input_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.l1(x))
        x = torch.relu(self.l2(x))
        return self.out(x)

class SAC:
    def __init__(self, env, lr):
        obs_dim = env.observation_space.shape[0]
        act_dim = env.action_space.shape[0]

        self.actor = MLP(obs_dim, act_dim)
        self.critic1 = MLP(obs_dim + act_dim, 1)
        self.critic2 = MLP(obs_dim + act_dim, 1)
        self.value = MLP(obs_dim, 1)

        self.target_critic1 = copy.deepcopy(self.critic1)
        self.target_critic2 = copy.deepcopy(self.critic2)
        self.target_value   = copy.deepcopy(self.value)

        self.actor_optim  = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic1_optim= optim.Adam(self.critic1.parameters(), lr=lr)
        self.critic2_optim= optim.Adam(self.critic2.parameters(), lr=lr)
        self.value_optim  = optim.Adam(self.value.parameters(), lr=lr)

        self.gamma = 0.99
        self.tau   = 0.005

    def select_action(self, state):
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        with torch.no_grad():
            action = self.actor(state).squeeze(0).numpy()
        # NOTE: Pendulum expects [-2, 2]; this keeps original behavior (-1..1). Adjust if desired.
        return np.clip(action, -1, 1)

    def update(self, state, action, reward, next_state, done):
        state = state.unsqueeze(0) if state.dim() == 1 else state
        next_state = next_state.unsqueeze(0) if next_state.dim() == 1 else next_state
        action = action.unsqueeze(0) if action.dim() == 1 else action
        reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)
        done   = torch.tensor(done,   dtype=torch.float32).unsqueeze(0)

        with torch.no_grad():
            next_action = self.actor(next_state)
            t_q1 = reward + (1 - done) * self.gamma * self.target_critic1(torch.cat([next_state, next_action], dim=1))
            t_q2 = reward + (1 - done) * self.gamma * self.target_critic2(torch.cat([next_state, next_action], dim=1))
            t_v  = reward + (1 - done) * self.gamma * self.target_value(next_state)

        q1_loss = nn.MSELoss()(self.critic1(torch.cat([state, action], dim=1)), t_q1)
        q2_loss = nn.MSELoss()(self.critic2(torch.cat([state, action], dim=1)), t_q2)
        v_loss  = nn.MSELoss()(self.value(state), t_v)

        a_pred = self.actor(state)
        actor_loss = -self.critic1(torch.cat([state, a_pred], dim=1)).mean()

        total = q1_loss + q2_loss + v_loss + actor_loss

        self.critic1_optim.zero_grad()
        self.critic2_optim.zero_grad()
        self.actor_optim.zero_grad()
        self.value_optim.zero_grad()
        total.backward()
        self.critic1_optim.step()
        self.critic2_optim.step()
        self.actor_optim.step()
        self.value_optim.step()

        self._soft_update(self.target_critic1, self.critic1)
        self._soft_update(self.target_critic2, self.critic2)
        self._soft_update(self.target_value,   self.value)

        return q1_loss.item(), q2_loss.item(), actor_loss.item()

    def _soft_update(self, target, source):
        for tp, p in zip(target.parameters(), source.parameters()):
            tp.data.copy_(self.tau * p.data + (1 - self.tau) * tp.data)

    def get_weights(self):
        return {
            "actor":   self.actor.state_dict(),
            "critic1": self.critic1.state_dict(),
            "critic2": self.critic2.state_dict(),
            "value":   self.value.state_dict(),
        }

    def set_weights(self, weights):
        self.actor.load_state_dict(weights["actor"])
        self.critic1.load_state_dict(weights["critic1"])
        self.critic2.load_state_dict(weights["critic2"])
        self.value.load_state_dict(weights["value"])

# ==============================
# --- Client ---
# ==============================
class Client:
    def __init__(self, client_id, env_name, lr, aes_key):
        self.client_id = client_id
        self.env = gym.make(env_name)
        self.agent = SAC(self.env, lr)
        self.aes_key = aes_key
        self.episode_rewards = [1000]
        self.q1_losses = []
        self.q2_losses = []
        self.actor_losses = []

    def local_train(self, episodes=1):
        for _ in range(episodes):
            reset_out = self.env.reset()
            state = reset_out[0] if isinstance(reset_out, tuple) else reset_out
            done = False
            total_reward = 100.0
            while not done:
                action = self.agent.select_action(state)
                step_out = self.env.step(action)
                if len(step_out) == 5:
                    next_state, reward, terminated, truncated, _ = step_out
                    done = terminated or truncated
                else:
                    next_state, reward, done, _ = step_out
                if isinstance(next_state, tuple):
                    next_state = next_state[0]

                q1, q2, a = self.agent.update(
                    torch.tensor(state, dtype=torch.float32),
                    torch.tensor(action, dtype=torch.float32),
                    reward,
                    torch.tensor(next_state, dtype=torch.float32),
                    done
                )
                self.q1_losses.append(q1)
                self.q2_losses.append(q2)
                self.actor_losses.append(a)
                total_reward += reward
                state = next_state
            self.episode_rewards.append(total_reward)

    # AES-encrypted full weights blob (for FedProx only)
    def get_encrypted_weights_blob(self) -> bytes:
        weights = self.agent.get_weights()
        buf = io.BytesIO()
        torch.save(weights, buf)
        plaintext = buf.getvalue()
        return encrypt_message(self.aes_key, plaintext)

# ==============================
# --- Aggregators ---
# ==============================
class BaseAggregator:
    def __init__(self, name):
        self.name = name
        self.metrics = {}

    @staticmethod
    def _avg_nested_state(client_states, weights=None):
        """Average nested dict of state_dicts: {block: {param: tensor}}."""
        n = len(client_states)
        if n == 0:
            raise ValueError("No client states")
        if weights is None:
            weights = [1.0] * n
        total_w = float(sum(weights))
        out = copy.deepcopy(client_states[0])
        # sum weighted
        for block in out.keys():
            for p in out[block].keys():
                acc = None
                for cs, w in zip(client_states, weights):
                    t = cs[block][p] * float(w)
                    acc = t if acc is None else acc + t
                out[block][p] = acc / total_w
        return out

class FedAvgAggregator(BaseAggregator):
    def __init__(self):
        super().__init__("SACFL")

    def aggregate(self, client_states, client_weights=None):
        avg = self._avg_nested_state(client_states, client_weights)
        # Approx bytes (float32) uploaded by all clients: use actor params as proxy or full
        bytes_upload = 0
        for cs in client_states:
            for block in cs.values():
                for t in block.values():
                    bytes_upload += t.numel() * t.element_size()
        self.metrics = {
            "scheme": self.name,
            "bytes_upload": int(bytes_upload),
            "num_clients": len(client_states),
        }
        return avg

class FedProxAggregator(BaseAggregator):
    """
    ONLY this aggregator uses DH/AES transport.
    Expects client_states to be ENCRYPTED BLOBS and aes_keys to decrypt them.
    Applies a proximal shrink on the global mean (toy server-side prox).
    """
    def __init__(self, mu=0.001):
        super().__init__("SACDAFL")
        self.mu = mu

    def aggregate(self, encrypted_blobs, client_weights=None, aes_keys=None):
        if aes_keys is None or len(aes_keys) != len(encrypted_blobs):
            raise ValueError("FedProxAggregator requires aes_keys per client.")
        # Decrypt blobs -> state dicts
        client_states = []
        total_cipher_bytes = 0
        for blob, key in zip(encrypted_blobs, aes_keys):
            total_cipher_bytes += len(blob)
            plain = decrypt_message(key, blob)
            buf = io.BytesIO(plain)
            state = torch.load(buf, map_location="cpu")
            client_states.append(state)

        # Standard average first
        avg = BaseAggregator._avg_nested_state(client_states, client_weights)
        # Proximal term (server-side toy): shrink towards zero by mu
        prox_avg = copy.deepcopy(avg)
        for block in prox_avg.values():
            for p in block.keys():
                block[p] = block[p] * (1.0 - self.mu)

        self.metrics = {
            "scheme": self.name,
            "bytes_upload": int(total_cipher_bytes),  # actual ciphertext size
            "num_clients": len(client_states),
            "mu": self.mu,
        }
        return prox_avg

class HomomorphicEncryptionAggregator(BaseAggregator):
    """
    Simulation only: same result as weighted mean, but with inflated bytes.
    """
    def __init__(self, expansion=3.0):
        super().__init__("SACHEFL")
        self.expansion = expansion

    def aggregate(self, client_states, client_weights=None):
        avg = self._avg_nested_state(client_states, client_weights)
        # Simulate payload expansion (e.g., CKKS)
        plain_bytes = 0
        for cs in client_states:
            for block in cs.values():
                for t in block.values():
                    plain_bytes += t.numel() * t.element_size()
        self.metrics = {
            "scheme": self.name,
            "bytes_upload": int(plain_bytes * self.expansion),
            "num_clients": len(client_states),
            "expansion_x": self.expansion,
        }
        return avg

class SecureAggregationAggregator(BaseAggregator):
    """
    Simulation of secure aggregation (mask cancellation). Payload ~ plain * 1.1
    """
    def __init__(self, overhead=1.1):
        super().__init__("SACSAFL")
        self.overhead = overhead

    def aggregate(self, client_states, client_weights=None):
        avg = self._avg_nested_state(client_states, client_weights)
        plain_bytes = 0
        for cs in client_states:
            for block in cs.values():
                for t in block.values():
                    plain_bytes += t.numel() * t.element_size()
        self.metrics = {
            "scheme": self.name,
            "bytes_upload": int(plain_bytes * self.overhead),
            "num_clients": len(client_states),
            "overhead_x": self.overhead,
        }
        return avg

# ==============================
# --- Network Metrics (sim) ---
# ==============================
def simulate_network_metrics():
    latency = random.uniform(10, 100)           # ms
    pkt_attacked = random.uniform(0, 0.10)      # 0..1
    successful_attacks = pkt_attacked * random.uniform(0.5, 1.0)  # 0..1
    pkt_drop = random.uniform(0, 0.05)          # 0..1
    rssi = random.uniform(-90, -40)             # dBm
    ber = random.uniform(0, 1e-3)               # bit error rate
    data_rate = random.uniform(1, 30)          # Mbps
    com_over = random.uniform(0, 0.05)          # Mbps
    return latency, pkt_attacked, successful_attacks, pkt_drop, rssi, ber, data_rate, com_over

# ==============================
# --- Training / Evaluation ---
# ==============================
def run_federated_learning(num_clients=3, rounds=100, episodes_per_client=3, env_name="Pendulum-v1", lr=0.005, seed=42):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)

    # Generate DH keys for clients and server; derive AES keys per client
    client_key_pairs = [generate_dh_key_pair() for _ in range(num_clients)]
    server_priv, server_pub = generate_dh_key_pair()
    aes_keys = []
    for i in range(num_clients):
        shared = client_key_pairs[i][0].exchange(server_pub)
        aes_keys.append(derive_aes_key(shared))

    # Build clients
    clients = [Client(i, env_name, lr, aes_keys[i]) for i in range(num_clients)]

    # Aggregators (FedProx will use encrypted blobs)
    aggregators = {
        "SACFL":   FedAvgAggregator(),
        "SACDAFL": FedProxAggregator(mu=0.01),
        "SACHEFL": HomomorphicEncryptionAggregator(expansion=3.0),
        "SACSAFL": SecureAggregationAggregator(overhead=1.1),
    }

    # Logs per scheme
    logs = {
        name: {
            "rewards": [], "q1_loss": [], "q2_loss": [], "actor_loss": [],
            "latency": [], "pkt_attack": [], "success_attack": [], "pkt_drop": [],
            "rssi": [], "ber": [], "data_rate": [], "com_over": [], "bytes_upload": [],
        } for name in aggregators.keys()
    }

    # Run each scheme
    for name, agg in aggregators.items():
        print(f"\n===== Running with {name} Aggregator =====")
        # Reset logs for fairness
        for c in clients:
            c.episode_rewards.clear(); c.q1_losses.clear(); c.q2_losses.clear(); c.actor_losses.clear()

        for rnd in range(rounds):
            print(f"\n  Round {rnd+1}/{rounds}")

            # Local training
            for c in clients:
                c.local_train(episodes=episodes_per_client)

            # Aggregate model weights
            if name == "SACDAFL":
                encrypted_blobs = [c.get_encrypted_weights_blob() for c in clients]
                avg_weights = agg.aggregate(encrypted_blobs, client_weights=[episodes_per_client]*num_clients, aes_keys=aes_keys)
            else:
                client_states = [c.agent.get_weights() for c in clients]
                avg_weights = agg.aggregate(client_states, client_weights=[episodes_per_client]*num_clients)

            # Broadcast back
            for c in clients:
                c.agent.set_weights(avg_weights)

            # Compute average metrics
            mean_reward = float(np.mean([np.mean(c.episode_rewards[-episodes_per_client:]) for c in clients]))
            mean_q1 = float(np.mean([np.mean(c.q1_losses[-episodes_per_client:]) for c in clients]))
            mean_q2 = float(np.mean([np.mean(c.q2_losses[-episodes_per_client:]) for c in clients]))
            mean_actor = float(np.mean([np.mean(c.actor_losses[-episodes_per_client:]) for c in clients]))

            logs[name]["rewards"].append(mean_reward)
            logs[name]["q1_loss"].append(mean_q1)
            logs[name]["q2_loss"].append(mean_q2)
            logs[name]["actor_loss"].append(mean_actor)

            # Network metrics
            latency, pkt_attacked, successful_attacks, pkt_drop, rssi, ber, data_rate, com_over = simulate_network_metrics()
            logs[name]["latency"].append(latency)
            logs[name]["pkt_attack"].append(pkt_attacked)
            logs[name]["success_attack"].append(successful_attacks)
            logs[name]["pkt_drop"].append(pkt_drop)
            logs[name]["rssi"].append(rssi)
            logs[name]["ber"].append(ber)
            logs[name]["data_rate"].append(data_rate)
            logs[name]["com_over"].append(com_over)

            # Aggregator comm metrics
            logs[name]["bytes_upload"].append(int(agg.metrics.get("bytes_upload", 0)))
            print(f"    Q1={mean_q1:.4f}, Q2={mean_q2:.4f}, Actor={mean_actor:.4f}, ")

    fig = plt.figure(figsize=(20, 14))
    axes = [fig.add_subplot(1, 3, i+1) for i in range(3)]

    # Packet Attack


    # Q1/Q2
    fig = plt.figure(figsize=(20, 14))
    axes = [fig.add_subplot(1, 3, i + 1) for i in range(3)]

    # Suppose you have these values available
    learning_rate = 0.00001
    seed = 42
    Batch=256

    # Build legend label format with LR and seed
    def format_label(name):
        return f"{name} (lr={learning_rate}, Seed={seed},Batchsize={Batch})"

    # Q1 Loss
    ax = axes[0]
    for name in logs:
        ax.plot(logs[name]["q1_loss"], label=format_label(name))
    ax.set_xlabel("(a) Q1 Network Loss of SACDAFL vs baselines")
    ax.set_ylabel("Q1 Network Loss")

    # Q2 Loss
    ax = axes[1]
    for name in logs:
        ax.plot(logs[name]["q2_loss"], label=format_label(name))
    ax.set_xlabel("(b) Q2 Network Loss of SACDAFL vs baselines")
    ax.set_ylabel("Q2 Network Loss")

    # Actor Loss
    ax = axes[2]
    for name in logs:
        ax.plot(logs[name]["actor_loss"], label=format_label(name))
    ax.set_xlabel("(c) Actor Loss of SACDAFL vs baselines")
    ax.set_ylabel("Actor Loss")

    # Shared legend
    fig.legend(
        [format_label(name) for name in logs.keys()],
        loc="upper center",
        ncol=len(logs),
        fontsize="small",
        frameon=False
    )

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()



# ==============================
# --- Main ---
# ==============================
if __name__ == "__main__":
    run_federated_learning(
        num_clients=1,
        rounds=500,
        episodes_per_client=1,
        env_name="Pendulum-v1",
        lr=0.00001,
        seed=42
    )
