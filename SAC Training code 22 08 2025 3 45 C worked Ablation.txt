import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import matplotlib.pyplot as plt
from collections import deque
from torch.distributions import Normal

# Environment Setup
env = gym.make('Pendulum-v1')

# Hyperparameters
LEARNING_RATES = [1e-3, 1e-4, 1e-5]  # Four different learning rates
BATCH_SIZES = [256]  # Corresponding batch sizes
#LEARNING_RATES = [1e-3, 1e-4, 1e-5, 1e-2]  # Four different learning rates
#BATCH_SIZES = [64, 128, 256, 512]


# Neural Network Models for SAC
class MLP(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=256):
        super(MLP, self).__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.layer2 = nn.Linear(hidden_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        return self.output(x)


# SAC Components
class SAC:
    def __init__(self, env, lr, batch_size):
        self.env = env
        self.lr = lr
        self.batch_size = batch_size

        self.actor = MLP(env.observation_space.shape[0], env.action_space.shape[0])
        self.critic1 = MLP(env.observation_space.shape[0] + env.action_space.shape[0], 1)
        self.critic2 = MLP(env.observation_space.shape[0] + env.action_space.shape[0], 1)
        self.target_critic1 = MLP(env.observation_space.shape[0] + env.action_space.shape[0], 1)
        self.target_critic2 = MLP(env.observation_space.shape[0] + env.action_space.shape[0], 1)
        self.value = MLP(env.observation_space.shape[0], 1)
        self.target_value = MLP(env.observation_space.shape[0], 1)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr)
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.lr)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=self.lr)

        self.gamma = 0.99  # Discount factor
        self.tau = 0.005  # Soft update for target networks
        self.target_entropy = -torch.prod(torch.Tensor(env.action_space.shape)).item()

    def update(self, state, action, reward, next_state, done):
        # Ensure state and next_state are 2D tensors (batch dimension)
        state = state.unsqueeze(0)  # Add batch dimension (1, 3)
        next_state = next_state.unsqueeze(0)  # Add batch dimension (1, 3)

        action = action.unsqueeze(0)  # Ensure action is in the shape (1, 1)

        # Q-update and Value-update (SAC algorithm steps)
        with torch.no_grad():
            next_action = self.actor(next_state)  # Get next action
            target_q1 = reward + (1 - done) * self.gamma * (
                self.target_critic1(torch.cat([next_state, next_action], dim=1)))
            target_q2 = reward + (1 - done) * self.gamma * (
                self.target_critic2(torch.cat([next_state, next_action], dim=1)))
            target_value = reward + (1 - done) * self.gamma * self.target_value(next_state)

        q1_loss = nn.MSELoss()(self.critic1(torch.cat([state, action], dim=1)), target_q1)
        q2_loss = nn.MSELoss()(self.critic2(torch.cat([state, action], dim=1)), target_q2)

        # Value function loss (SAC)
        value_loss = nn.MSELoss()(self.value(state), target_value)

        # Update the critics
        self.critic1_optimizer.zero_grad()
        self.critic2_optimizer.zero_grad()
        q1_loss.backward()
        q2_loss.backward()
        self.critic1_optimizer.step()
        self.critic2_optimizer.step()

        # Policy update
        actor_loss = -self.critic1(torch.cat([state, self.actor(state)], dim=1)).mean()

        # Update the actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Soft update of target networks
        self.soft_update(self.target_critic1, self.critic1)
        self.soft_update(self.target_critic2, self.critic2)
        self.soft_update(self.target_value, self.value)

        return q1_loss.item(), q2_loss.item(), actor_loss.item(), value_loss.item()

    def soft_update(self, target, source):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

    def select_action(self, state):
        state = np.array(state)  # Ensure it's a numpy array
        state = torch.Tensor(state).unsqueeze(0)  # Add batch dimension (1, 3)

        # Select action
        action = self.actor(state).detach()

        # Ensure action is in the correct shape (1,)
        action = action.squeeze(0).numpy()  # Remove unnecessary dimensions
        return action


# Training loop
# Training loop
def train_sac(lr, batch_size, episodes=20):
    agent = SAC(env, lr, batch_size)
    episode_rewards = []
    q1_losses, q2_losses, actor_losses, value_losses = [], [], [], []

    for episode in range(episodes):
        # Reset environment and handle state and info correctly
        reset_output = env.reset()  # This may return either a state or (state, info)

        # Check if the reset_output has two elements (state and info)
        if isinstance(reset_output, tuple):
            state, info = reset_output
        else:
            state = reset_output
            info = {}  # Assign an empty dict if info is not returned

        done = False
        total_reward = 0

        while not done:
            action = agent.select_action(state)
            step_output = env.step(action)  # This may return 4 or 5 values

            # Check if step_output has 4 or 5 elements
            if len(step_output) == 5:
                next_state, reward, done, truncated, info = step_output
                done = done or truncated  # Account for truncation
            else:
                next_state, reward, done, info = step_output

            q1_loss, q2_loss, actor_loss, value_loss = agent.update(
                torch.Tensor(state),
                torch.Tensor(action),
                reward,
                torch.Tensor(next_state),
                done
            )

            state = next_state
            total_reward += reward

            q1_losses.append(q1_loss)
            q2_losses.append(q2_loss)
            actor_losses.append(actor_loss)
            value_losses.append(value_loss)

        episode_rewards.append(total_reward)

        # Print losses and rewards for the episode
        print(f"Episode {episode}/{episodes} | Total Reward: {total_reward:.2f}")
        print(
            f"Q1 Loss: {q1_losses[-1]:.4f}, Q2 Loss: {q2_losses[-1]:.4f}, Actor Loss: {actor_losses[-1]:.4f}, Value Loss: {value_losses[-1]:.4f}")
        print("-" * 50)

    return episode_rewards, q1_losses, q2_losses, actor_losses, value_losses


# Run experiments for different learning rates and batch sizes
results = {}
for lr in LEARNING_RATES:
    for batch_size in BATCH_SIZES:
        print(f"Training with lr={lr} and Batchsize={batch_size}")
        rewards, q1_loss, q2_loss, actor_loss, value_loss = train_sac(lr, batch_size)
        results[(lr, batch_size)] = (rewards, q1_loss, q2_loss, actor_loss, value_loss)

# Plot results with 1 row, 3 subplots
fig, axs = plt.subplots(1, 3, figsize=(24, 6))

# ---------------- Plot 1: Learning Curves ----------------
for (lr, batch_size), (rewards, _, _, _, _) in results.items():
    axs[0].plot(rewards, label=f'lr={lr}, B={batch_size}')

axs[0].set_xlabel('(a) Episodes', fontsize=21)
axs[0].set_ylabel('Reward', fontsize=21)
axs[0].legend(fontsize=15, loc='lower right')
axs[0].tick_params(axis='x', labelsize=21)
axs[0].tick_params(axis='y', labelsize=21)

# ---------------- Plot 2: Q1 and Q2 Loss ----------------
for (lr, batch_size), (_, q1_loss, q2_loss, _, _) in results.items():
    axs[1].plot(q1_loss, label=f'Q1 lr={lr}, B={batch_size}')
    axs[1].plot(q2_loss, label=f'Q2 lr={lr}, B={batch_size}')

axs[1].set_xlabel('(b) Updates', fontsize=21)
axs[1].set_ylabel('Q1 vs Q2 Loss', fontsize=21)
axs[1].legend(fontsize=14, loc='upper right')
axs[1].tick_params(axis='x', labelsize=21)
axs[1].tick_params(axis='y', labelsize=21)

# ---------------- Plot 3: Actor Loss ----------------
for (lr, batch_size), (_, _, _, actor_loss, _) in results.items():
    axs[2].plot(actor_loss, label=f'lr={lr}, B={batch_size}')

axs[2].set_xlabel('(c) Updates', fontsize=21)
axs[2].set_ylabel('Actor Loss', fontsize=21)
axs[2].legend(fontsize=15, loc='upper left')
axs[2].tick_params(axis='x', labelsize=21)
axs[2].tick_params(axis='y', labelsize=21)

# ---------------- Final layout ----------------
plt.tight_layout()
plt.show()

